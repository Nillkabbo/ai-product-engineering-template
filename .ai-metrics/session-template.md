# AI Session Report

## Task Reference
- **Task File**: [Link to tasks/TASK_XXX.md]
- **Task Goal**: [Brief description]
- **Task ID**: [e.g., TASK_001]

## Model Configuration

### From Task File
- **Primary Model** (from task): [Auto-filled from task, e.g., Claude Sonnet 4]
- **Fallback Model** (from task): [Auto-filled from task, e.g., GPT-4]
- **Task Complexity**: [Auto-filled: Simple/Medium/Complex]
- **Task Type**: [Auto-filled: Frontend/Backend/etc.]

### Actual Model Used
- **Model Name**: [e.g., Claude Sonnet 4]
- **Model Version**: [e.g., claude-sonnet-4.5-20250514]
- **Model Match**: [✅ Matches Primary / ⚠️ Using Fallback / ❌ Different Model]

### Model Selection Validation
- [ ] Model used matches task primary or fallback
- [ ] If different model used, rationale documented below

### Rationale for Model Change (if applicable)
[Explain why a different model was used if Model Match is ❌ or ⚠️]
[Examples: API rate limit, budget constraint, model unavailable, model struggled with subtask]

## Usage Metrics
- **Tokens Input**: [e.g., 15,000]
- **Tokens Output**: [e.g., 5,000]
- **Total Tokens**: [e.g., 20,000]
- **Estimated Cost**: [e.g., $0.60]
- **Session Duration**: [e.g., 45 minutes]

## Outcome
- **Result**: [✅ Success / ⚠️ Partial Success / ❌ Failed]
- **Acceptance Criteria Met**: [X/Y criteria met]
- **Tests Status**: [✅ All Pass / ⚠️ Some Fail / ❌ Many Fail]

## Session Notes
[Detailed notes about the implementation session]

### What Went Well
- [Positive observations]

### Challenges
- [Issues encountered and how resolved]

### Deviations from Plan
- [Any changes from the original task plan]

## Quality Assessment
- **Code Quality**: [Excellent / Good / Acceptable / Needs Improvement]
- **Architecture Compliance**: [✅ No Violations / ⚠️ Minor / ❌ Major]
- **Test Coverage**: [High / Medium / Low]

## Recommendations for Future Tasks
[Learnings to apply to similar tasks]
[Model performance observations]
